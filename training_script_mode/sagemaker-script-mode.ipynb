{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Your Own Model with SageMaker Script Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how you can bring your own model by using custom training and inference scripts, similar to those you would use outside of SageMaker, with SageMaker's prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost.\n",
    "\n",
    "SageMaker Script Mode is flexible so you'll also be seeing examples of how to include your own dependencies, such as a custom Python library, in your training and inference.\n",
    "\n",
    "The following diagram provides a solution overview:\n",
    "\n",
    "<img title=\"SageMaker Script Mode\" alt=\"Solution diagram\" src=\"solution-diagram.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along, you need to create an IAM role, SageMaker Notebook instance, and S3 bucket. You may click on the CloudFormation button which will create the aforementioned resources and clone the `amazon-sagemaker-examples` GitHub repo into the notebook instance. [![Launch Stack](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home#/stacks/new?stackName=ScriptModeDemo&templateURL=https://script-mode-blog.s3.amazonaws.com/script-mode-blog-cfn.yml). Give the S3bucket a unique name; you can also give the CloudFormation stack and notebook unique names such as \"script mode\". You can leave the other default settings in the CloudFormation template.\n",
    "\n",
    "Once the SageMaker Notebook instance is created, choose `conda_python3` as the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.129.0)\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (0.0.post2)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.26.56)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (22.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.56 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.56)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.56->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sagemaker sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.serializers import NumpySerializer, JSONSerializer, CSVSerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer, JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from generate_synthetic_housing_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SageMaker version is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.24.1\":\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.24.1\"])\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Useful SageMaker variables\n",
    "try:\n",
    "    # You're using a SageMaker notebook\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # You're using a notebook somewhere else\n",
    "    print(\"Setting role and SageMaker session manually...\")\n",
    "    bucket = \"bobby-demo\"\n",
    "    region = \"us-west-2\"\n",
    "\n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    sagemaker_execution_role_name = (\n",
    "        \"AmazonSageMaker-ExecutionRole-20200630T141851\"  # Change this to your role name\n",
    "    )\n",
    "    role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    boto3.setup_default_session(region_name=region, profile_name=\"default\")\n",
    "    sess = sagemaker.Session(sagemaker_client=sagemaker_client, default_bucket=bucket)\n",
    "\n",
    "# Local data paths\n",
    "train_dir = os.path.join(os.getcwd(), \"data/train\")\n",
    "test_dir = os.path.join(os.getcwd(), \"data/test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Data paths in S3\n",
    "s3_prefix = \"script-mode-workflow\"\n",
    "csv_s3_prefix = f\"{s3_prefix}/csv\"\n",
    "csv_s3_uri = f\"s3://{bucket}/{s3_prefix}/csv\"\n",
    "numpy_train_s3_prefix = f\"{s3_prefix}/numpy/train\"\n",
    "numpy_train_s3_uri = f\"s3://{bucket}/{numpy_train_s3_prefix}\"\n",
    "numpy_test_s3_prefix = f\"{s3_prefix}/numpy/test\"\n",
    "numpy_test_s3_uri = f\"s3://{bucket}/{numpy_test_s3_prefix}\"\n",
    "csv_train_s3_uri = f\"{csv_s3_uri}/train\"\n",
    "csv_test_s3_uri = f\"{csv_s3_uri}/test\"\n",
    "\n",
    "# Enable Local Mode training\n",
    "enable_local_mode_training = False\n",
    "\n",
    "# Endpoint names\n",
    "sklearn_endpoint_name = \"randomforestregressor-endpoint\"\n",
    "pytorch_endpoint_name = \"pytorch-endpoint\"\n",
    "xgboost_endpoint_name = \"xgboost-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./local_mode_setup.sh: line 45: docker: command not found\n",
      "./local_mode_setup.sh: line 47: docker: command not found\n",
      "./local_mode_setup.sh: line 51: sudo: command not found\n",
      "./local_mode_setup.sh: line 54: docker: command not found\n",
      "./local_mode_setup.sh: line 55: ip: command not found\n",
      "./local_mode_setup.sh: line 56: ip: command not found\n",
      "./local_mode_setup.sh: line 59: sudo: command not found\n",
      "./local_mode_setup.sh: line 60: sudo: command not found\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/local_mode_setup.sh\n",
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/daemon.json\n",
    "!/bin/bash ./local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Synthetic Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the examples below, we'll be generating a synthetic housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = generate_houses(1506)\n",
    "\n",
    "# Get training columns\n",
    "train_cols = list(df.columns)\n",
    "del train_cols[-1]\n",
    "train_cols\n",
    "\n",
    "# Split data\n",
    "training_index = math.floor(0.8 * df.shape[0])\n",
    "x_train, y_train = df[train_cols][:training_index], df.PRICE[:training_index]\n",
    "x_test, y_test = df[train_cols][training_index:], df.PRICE[training_index:]\n",
    "\n",
    "# Scale price\n",
    "y_train = y_train / 100000\n",
    "y_test = y_test / 100000\n",
    "\n",
    "# Standardize data\n",
    "x_train_np = StandardScaler().fit_transform(x_train)\n",
    "x_test_np = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>4880.707047</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>2372.074001</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999</td>\n",
       "      <td>2961.434069</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988</td>\n",
       "      <td>2803.218334</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987</td>\n",
       "      <td>2530.875697</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0        2004  4880.707047             6            3.0       0.90   \n",
       "1        1991  2372.074001             4            2.5       1.02   \n",
       "2        1999  2961.434069             2            2.0       1.02   \n",
       "3        1988  2803.218334             5            1.5       1.02   \n",
       "4        1987  2530.875697             4            2.0       0.87   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH  DECK  \n",
       "0              0            1     1  \n",
       "1              2            0     1  \n",
       "2              0            1     1  \n",
       "3              1            1     1  \n",
       "4              3            0     1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7.75606\n",
       "1    3.38611\n",
       "2    4.09515\n",
       "3    3.68282\n",
       "4    3.47681\n",
       "Name: PRICE, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearrange dataframe for SageMaker training and scale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data=x_train_np)\n",
    "train_df.columns = x_train.columns\n",
    "train_df[\"PRICE\"] = y_train / 100000\n",
    "first_col = train_df.pop(\"PRICE\")\n",
    "train_df.insert(0, \"PRICE\", first_col)\n",
    "\n",
    "test_df = pd.DataFrame(data=x_test_np)\n",
    "test_df.columns = x_test.columns\n",
    "test_df[\"PRICE\"] = y_test.reset_index(drop=True) / 100000\n",
    "first_col = test_df.pop(\"PRICE\")\n",
    "test_df.insert(0, \"PRICE\", first_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as both CSV and Numpy data types to demonstrate data type flexibility in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "train_df.to_csv(f\"{train_dir}/train.csv\", header=False, index=False)\n",
    "test_df.to_csv(f\"{test_dir}/test.csv\", header=False, index=False)\n",
    "\n",
    "# Save as Numpy\n",
    "np.save(os.path.join(train_dir, \"x_train.npy\"), x_train_np)\n",
    "np.save(os.path.join(test_dir, \"x_test.npy\"), x_test_np)\n",
    "np.save(os.path.join(train_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(test_dir, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_resource_bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, \"train.csv\")).upload_file(\n",
    "    \"data/train/train.csv\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, \"test.csv\")).upload_file(\"data/test/test.csv\")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"x_train.npy\")).upload_file(\n",
    "    \"data/train/x_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"y_train.npy\")).upload_file(\n",
    "    \"data/train/y_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"x_test.npy\")).upload_file(\n",
    "    \"data/test/x_test.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"y_test.npy\")).upload_file(\n",
    "    \"data/test/y_test.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first \"level\" of script mode is the ability to define your own training job, model, and inference process without any dependencies. This is done using a customized python script and pointing that script as the \"entry point\" when defining your SageMaker training estimator. There is no \"out-of-the-box\" random forest algorithm on SageMaker, but there is support for scikit-learn containers which does have random forest implementations, including regressors and classifiers. Here, we demonstrate the implementation of a custom random forest regressor to predict housing prices using our synthetic housing data set.\n",
    "\n",
    "Script Mode in SageMaker allows you to take control of the training and inference process without having to go through the trouble of creating and maintaining your own docker containers. For example, if you want to use a scikit-learn algorithm, just use the AWS-provided scikit-learn container and pass it your own training and inference code. On your behalf, the SageMaker Python SDK will package this entry point script (which can be your training and/or inference code), upload it to S3, and set two environment variables that are read at runtime and load the custom training and inference functions from the entry point script. These two environment variables are `SAGEMAKER_SUBMIT_DIRECTORY` which is set to the S3 path of the package and `SAGEMAKER_PROGRAM` which is set to the name of the script (which in our case is `train_deploy_scikitlearn_without_dependencies.py`).\n",
    "\n",
    "The process is the same if you want to use an XGBoost model (use the XGBoost container) or a custom PyTorch model (use the PyTorch container). Since you're passing in your own script (which is why we call it \"script mode\"), you get to define the model, the training process, and the inference process as well.\n",
    "\n",
    "Below we include an entry point script called `train_deploy_scikitlearn_without_dependencies.py` which contains our custom training and inference code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.24.1)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.129.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (0.0.post2)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.26.56)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (22.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.56 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.56)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.56->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.24.1\n",
      "    Uninstalling sagemaker-2.24.1:\n",
      "      Successfully uninstalled sagemaker-2.24.1\n",
      "Successfully installed sagemaker-2.129.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sagemaker sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: randomforestregressor-model-2023-01-25-16-28-11-807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-25 16:28:12 Starting - Starting the training job...\n",
      "2023-01-25 16:28:29 Starting - Preparing the instances for training......\n",
      "2023-01-25 16:29:23 Downloading - Downloading input data...\n",
      "2023-01-25 16:30:08 Training - Downloading the training image..\u001b[34m2023-01-25 16:30:18,063 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2023-01-25 16:30:18,066 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:30:18,072 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-25 16:30:18,296 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:30:18,305 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:30:18,314 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:30:18,321 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_depth\": 20,\n",
      "        \"n_estimators\": 120,\n",
      "        \"n_jobs\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"randomforestregressor-model-2023-01-25-16-28-11-807\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-512949343409/randomforestregressor-model-2023-01-25-16-28-11-807/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy_scikitlearn_without_dependencies\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy_scikitlearn_without_dependencies.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"max_depth\":20,\"n_estimators\":120,\"n_jobs\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy_scikitlearn_without_dependencies.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy_scikitlearn_without_dependencies\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-512949343409/randomforestregressor-model-2023-01-25-16-28-11-807/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"max_depth\":20,\"n_estimators\":120,\"n_jobs\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"randomforestregressor-model-2023-01-25-16-28-11-807\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-512949343409/randomforestregressor-model-2023-01-25-16-28-11-807/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy_scikitlearn_without_dependencies\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy_scikitlearn_without_dependencies.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--max_depth\",\"20\",\"--n_estimators\",\"120\",\"--n_jobs\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=20\u001b[0m\n",
      "\u001b[34mSM_HP_N_ESTIMATORS=120\u001b[0m\n",
      "\u001b[34mSM_HP_N_JOBS=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python train_deploy_scikitlearn_without_dependencies.py --max_depth 20 --n_estimators 120 --n_jobs 4\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34mTraining the classifier\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 120 out of 120 | elapsed:    0.2s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 120 out of 120 | elapsed:    0.0s finished\u001b[0m\n",
      "\u001b[34mScore: 0.9525191822688592\u001b[0m\n",
      "\u001b[34m2023-01-25 16:30:19,493 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-25 16:30:35 Uploading - Uploading generated training model\n",
      "2023-01-25 16:30:35 Completed - Training job completed\n",
      "Training seconds: 73\n",
      "Billable seconds: 73\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\"max_depth\": 20, \"n_jobs\": 4, \"n_estimators\": 120}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local\"\n",
    "    inputs = {\"train\": f\"file://{train_dir}\", \"test\": f\"file://{test_dir}\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.c5.xlarge\"\n",
    "    inputs = {\"train\": csv_train_s3_uri, \"test\": csv_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_scikitlearn_without_dependencies.py\",\n",
    "    \"source_dir\": \"scikitlearn_script\",\n",
    "    \"framework_version\": \"1.0-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"randomforestregressor-model\",\n",
    "}\n",
    "\n",
    "estimator = SKLearn(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the estimator finishes training, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: randomforestregressor-model-2023-01-25-16-30-55-845\n",
      "INFO:sagemaker:Creating endpoint-config with name randomforestregressor-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name randomforestregressor-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=sklearn_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    sklearn_predictor = estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=sklearn_endpoint_name\n",
    "    )\n",
    "else:\n",
    "    sklearn_predictor = Predictor(\n",
    "        endpoint_name=\"randomforestregressor-endpoint\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=NumpySerializer(),\n",
    "        deserializer=NumpyDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the SageMaker endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00100000e+03, 2.95349534e+03, 4.00000000e+00, 1.50000000e+00,\n",
       "       9.80000000e-01, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.18585150e-05, 8.28684425e-05, 8.09093251e-05, 8.16602900e-05,\n",
       "       8.16602900e-05, 8.24719075e-05, 8.19989508e-05, 8.31089075e-05,\n",
       "       8.23830242e-05, 8.17438317e-05, 8.17412975e-05, 8.21355925e-05,\n",
       "       8.06022810e-05, 8.16628242e-05, 8.21673700e-05, 8.17412975e-05,\n",
       "       8.12814143e-05, 8.10113543e-05, 8.09092410e-05, 8.09223660e-05,\n",
       "       8.21355925e-05, 8.08191043e-05, 8.17039658e-05, 8.10439260e-05,\n",
       "       8.15642025e-05, 8.11468135e-05, 8.10939885e-05, 8.09456876e-05,\n",
       "       8.24719075e-05, 8.05686235e-05, 8.25255008e-05, 8.20049542e-05,\n",
       "       8.17438317e-05, 8.05711576e-05, 8.17565658e-05, 8.09223660e-05,\n",
       "       8.09067068e-05, 8.13087768e-05, 8.18585150e-05, 8.25255008e-05,\n",
       "       8.25255008e-05, 8.08137058e-05, 8.20024200e-05, 8.23830242e-05,\n",
       "       8.23830242e-05, 8.17412975e-05, 8.33042792e-05, 8.11910518e-05,\n",
       "       8.08762851e-05, 8.11910518e-05, 8.23301292e-05, 8.17529493e-05,\n",
       "       8.20084018e-05, 8.15057408e-05, 8.21869358e-05, 8.27185217e-05,\n",
       "       8.15062750e-05, 8.26601017e-05, 8.09092410e-05, 8.09456876e-05,\n",
       "       8.18585150e-05, 8.21355925e-05, 8.25255008e-05, 8.25318250e-05,\n",
       "       8.21869358e-05, 8.25255008e-05, 8.10113543e-05, 8.17108758e-05,\n",
       "       8.23360575e-05, 8.05686235e-05, 8.10939885e-05, 8.04101766e-05,\n",
       "       8.22562533e-05, 8.11910518e-05, 8.25710200e-05, 8.08465751e-05,\n",
       "       8.20009917e-05, 8.16950892e-05, 8.22003258e-05, 8.20009917e-05,\n",
       "       8.09093251e-05, 8.06483501e-05, 8.21869358e-05, 8.28092233e-05,\n",
       "       8.17412975e-05, 8.19719983e-05, 8.20386350e-05, 8.20402625e-05,\n",
       "       8.25710200e-05, 8.22922392e-05, 8.20009917e-05, 8.22866750e-05,\n",
       "       8.24851792e-05, 8.18464425e-05, 8.05914949e-05, 8.23811225e-05,\n",
       "       8.20009917e-05, 8.20991792e-05, 8.04040560e-05, 8.18623283e-05,\n",
       "       8.16950892e-05, 8.09741710e-05, 8.28684425e-05, 8.19255918e-05,\n",
       "       8.27193208e-05, 8.20386350e-05, 8.20402625e-05, 8.10439260e-05,\n",
       "       8.17412975e-05, 8.25255008e-05, 8.28092233e-05, 8.17565658e-05,\n",
       "       8.23811225e-05, 8.09092410e-05, 8.19395225e-05, 8.20687575e-05,\n",
       "       8.16602900e-05, 8.25255008e-05, 8.20386350e-05, 8.17412975e-05,\n",
       "       8.18585150e-05, 8.16298683e-05, 8.05382018e-05, 8.09456876e-05,\n",
       "       8.27947367e-05, 8.23835242e-05, 8.11468135e-05, 8.11910518e-05,\n",
       "       8.22866750e-05, 8.23360575e-05, 8.21673700e-05, 8.27185217e-05,\n",
       "       8.24338358e-05, 8.20024200e-05, 8.20009917e-05, 8.21355925e-05,\n",
       "       8.15057408e-05, 8.16602900e-05, 8.06544708e-05, 8.21869358e-05,\n",
       "       8.21869358e-05, 8.22866750e-05, 8.20049542e-05, 8.21869358e-05,\n",
       "       8.25710200e-05, 8.23835242e-05, 8.23811225e-05, 8.08216385e-05,\n",
       "       8.23811225e-05, 8.20402625e-05, 8.17412975e-05, 8.22866750e-05,\n",
       "       8.11910518e-05, 8.20968675e-05, 8.09093251e-05, 8.20009917e-05,\n",
       "       8.18585150e-05, 8.21977917e-05, 8.09067068e-05, 8.06464618e-05,\n",
       "       8.10939885e-05, 8.18919617e-05, 8.20009917e-05, 8.25710200e-05,\n",
       "       8.09067068e-05, 8.25710200e-05, 8.21301650e-05, 8.17108758e-05,\n",
       "       8.22003258e-05, 8.05320433e-05, 8.21355925e-05, 8.21869358e-05,\n",
       "       8.21977917e-05, 8.06022810e-05, 8.27947367e-05, 8.20009917e-05,\n",
       "       8.22562533e-05, 8.21869358e-05, 8.22003258e-05, 8.26601017e-05,\n",
       "       8.26601017e-05, 8.28092233e-05, 8.20386350e-05, 8.19395225e-05,\n",
       "       8.24851792e-05, 8.09456876e-05, 8.08191043e-05, 8.08316943e-05,\n",
       "       8.19003383e-05, 8.22386383e-05, 8.23830242e-05, 8.16628242e-05,\n",
       "       8.17412975e-05, 8.09093251e-05, 8.28684425e-05, 8.19395225e-05,\n",
       "       8.25710200e-05, 8.16268551e-05, 8.20024200e-05, 8.15442850e-05,\n",
       "       8.08401474e-05, 8.09456876e-05, 8.15057408e-05, 8.17108758e-05,\n",
       "       8.21869358e-05, 8.09223660e-05, 8.11910518e-05, 8.22847142e-05,\n",
       "       8.16602900e-05, 8.09456876e-05, 8.25710200e-05, 8.17108758e-05,\n",
       "       8.16602900e-05, 8.22386383e-05, 8.10939885e-05, 8.23830242e-05,\n",
       "       8.27185217e-05, 8.08216385e-05, 8.25710200e-05, 8.21893842e-05,\n",
       "       8.14305360e-05, 8.20009917e-05, 8.23360575e-05, 8.26601017e-05,\n",
       "       8.26601017e-05, 8.19395225e-05, 8.11468135e-05, 8.22866750e-05,\n",
       "       8.09456876e-05, 8.25318250e-05, 8.24647300e-05, 8.12814143e-05,\n",
       "       8.21673700e-05, 8.20009917e-05, 8.28684425e-05, 8.20386350e-05,\n",
       "       8.20009917e-05, 8.25255008e-05, 8.15642025e-05, 8.19395225e-05,\n",
       "       8.27193208e-05, 8.16950892e-05, 8.10043368e-05, 8.09456876e-05,\n",
       "       8.22562533e-05, 8.19810433e-05, 8.17565658e-05, 8.22562533e-05,\n",
       "       8.20009917e-05, 8.22847142e-05, 8.17438317e-05, 8.12431101e-05,\n",
       "       8.11910518e-05, 8.25710200e-05, 8.21869358e-05, 8.05686235e-05,\n",
       "       8.24338358e-05, 8.15946242e-05, 8.23360575e-05, 8.20009917e-05,\n",
       "       8.23301292e-05, 8.21017133e-05, 8.25255008e-05, 8.18375733e-05,\n",
       "       8.26601017e-05, 8.27185217e-05, 8.21355925e-05, 8.10919393e-05,\n",
       "       8.21977917e-05, 8.21869358e-05, 8.16375960e-05, 8.20386350e-05,\n",
       "       8.09093251e-05, 8.22562533e-05, 8.10919393e-05, 8.19395225e-05,\n",
       "       8.21673700e-05, 8.04828483e-05, 8.19543833e-05, 8.33883700e-05,\n",
       "       8.20386350e-05, 8.23835242e-05, 8.07668485e-05, 8.23360575e-05,\n",
       "       8.19255918e-05, 8.13762250e-05, 8.23830242e-05, 8.12814143e-05,\n",
       "       8.18585150e-05, 8.14632775e-05, 8.07295435e-05, 8.15946242e-05,\n",
       "       8.09093251e-05, 8.20024200e-05, 8.16950892e-05, 8.17039658e-05,\n",
       "       8.20009917e-05, 8.18919617e-05, 8.22922392e-05, 8.27193208e-05,\n",
       "       8.22562533e-05, 8.10714876e-05])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_predictor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second \"level\" of script mode is the ability to modularize and logically organize your custom training jobs, models, and inference processes.\n",
    "\n",
    "Sometimes keeping all your code in one Python file can be unwieldy. Script Mode gives you the flexibility to parse out your code into multiple Python files. To illustrate this feature we build a custom PyTorch model and logically separate the model definition from the the training and inference logic. This is done by stipulating the source directory when defining your SageMaker training estimator (illustrated below). Once again, the model is not supported \"out-of-the-box\", but the PyTorch framework is and can be leveraged in the same manner as scikit-learn was in the previous example.\n",
    "\n",
    "In this PyTorch example, we want to separate the actual neural network definition from the rest of the code by putting it into its own file as demonstrated in the `pytorch_script/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-model-2023-01-25-16-36-26-892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-25 16:36:27 Starting - Starting the training job...\n",
      "2023-01-25 16:36:56 Starting - Preparing the instances for training......\n",
      "2023-01-25 16:37:48 Downloading - Downloading input data...\n",
      "2023-01-25 16:38:18 Training - Downloading the training image...\n",
      "2023-01-25 16:38:49 Uploading - Uploading generated training model\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,426 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,429 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,438 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,440 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,628 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,640 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,651 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:43,661 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"epochs\": 25,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-model-2023-01-25-16-36-26-892\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-512949343409/pytorch-model-2023-01-25-16-36-26-892/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy_pytorch_without_dependencies\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy_pytorch_without_dependencies.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":25,\"learning_rate\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy_pytorch_without_dependencies.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy_pytorch_without_dependencies\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-512949343409/pytorch-model-2023-01-25-16-36-26-892/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":25,\"learning_rate\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-model-2023-01-25-16-36-26-892\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-512949343409/pytorch-model-2023-01-25-16-36-26-892/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy_pytorch_without_dependencies\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy_pytorch_without_dependencies.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"25\",\"--learning_rate\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=25\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_deploy_pytorch_without_dependencies.py --batch_size 128 --epochs 25 --learning_rate 0.01\u001b[0m\n",
      "\u001b[34mx train: (1204, 8), y train: (1204,)\u001b[0m\n",
      "\u001b[34mx test: (302, 8), y test: (302,)\u001b[0m\n",
      "\u001b[34mbatch_size = 128, epochs = 25, learning rate = 0.01\u001b[0m\n",
      "\u001b[34m[2023-01-25 16:38:44.324 algo-1:27 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-25 16:38:44.324 algo-1:27 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-25 16:38:44.324 algo-1:27 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-25 16:38:44.324 algo-1:27 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-25 16:38:44.325 algo-1:27 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-25 16:38:44.325 algo-1:27 INFO hook.py:461] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mepoch: 1 -> loss: 9.812063217163086\u001b[0m\n",
      "\u001b[34mepoch: 2 -> loss: 5.392348289489746\u001b[0m\n",
      "\u001b[34mepoch: 3 -> loss: 1.9229687452316284\u001b[0m\n",
      "\u001b[34mepoch: 4 -> loss: 1.6343133449554443\u001b[0m\n",
      "\u001b[34mepoch: 5 -> loss: 1.6010115146636963\u001b[0m\n",
      "\u001b[34mepoch: 6 -> loss: 2.142791509628296\u001b[0m\n",
      "\u001b[34mepoch: 7 -> loss: 1.4394766092300415\u001b[0m\n",
      "\u001b[34mepoch: 8 -> loss: 1.5406466722488403\u001b[0m\n",
      "\u001b[34mepoch: 9 -> loss: 1.7243475914001465\u001b[0m\n",
      "\u001b[34mepoch: 10 -> loss: 1.511121153831482\u001b[0m\n",
      "\u001b[34mepoch: 11 -> loss: 1.314211130142212\u001b[0m\n",
      "\u001b[34mepoch: 12 -> loss: 1.3748958110809326\u001b[0m\n",
      "\u001b[34mepoch: 13 -> loss: 0.9847291111946106\u001b[0m\n",
      "\u001b[34mepoch: 14 -> loss: 1.1319645643234253\u001b[0m\n",
      "\u001b[34mepoch: 15 -> loss: 1.139477252960205\u001b[0m\n",
      "\u001b[34mepoch: 16 -> loss: 1.0381397008895874\u001b[0m\n",
      "\u001b[34mepoch: 17 -> loss: 0.6563830971717834\u001b[0m\n",
      "\u001b[34mepoch: 18 -> loss: 0.796422004699707\u001b[0m\n",
      "\u001b[34mepoch: 19 -> loss: 0.3713085949420929\u001b[0m\n",
      "\u001b[34mepoch: 20 -> loss: 0.6782872080802917\u001b[0m\n",
      "\u001b[34mepoch: 21 -> loss: 0.579037070274353\u001b[0m\n",
      "\u001b[34mepoch: 22 -> loss: 0.45041343569755554\u001b[0m\n",
      "\u001b[34mepoch: 23 -> loss: 0.3893235921859741\u001b[0m\n",
      "\u001b[34mepoch: 24 -> loss: 0.35650235414505005\u001b[0m\n",
      "\u001b[34mepoch: 25 -> loss: 0.2811172604560852\u001b[0m\n",
      "\u001b[34mTest MSE: 0.31339323019002047\u001b[0m\n",
      "\u001b[34mCreated a folder at /opt/ml/model/code/!\u001b[0m\n",
      "\u001b[34mSaving models files to /opt/ml/model/code/\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 1 -> loss: 9.812063217163086\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 2 -> loss: 5.392348289489746\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 3 -> loss: 1.9229687452316284\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 4 -> loss: 1.6343133449554443\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 5 -> loss: 1.6010115146636963\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 6 -> loss: 2.142791509628296\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 7 -> loss: 1.4394766092300415\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 8 -> loss: 1.5406466722488403\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 9 -> loss: 1.7243475914001465\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 10 -> loss: 1.511121153831482\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 11 -> loss: 1.314211130142212\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 12 -> loss: 1.3748958110809326\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 13 -> loss: 0.9847291111946106\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 14 -> loss: 1.1319645643234253\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 15 -> loss: 1.139477252960205\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 16 -> loss: 1.0381397008895874\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 17 -> loss: 0.6563830971717834\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 18 -> loss: 0.796422004699707\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 19 -> loss: 0.3713085949420929\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 20 -> loss: 0.6782872080802917\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 21 -> loss: 0.579037070274353\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 22 -> loss: 0.45041343569755554\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 23 -> loss: 0.3893235921859741\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 24 -> loss: 0.35650235414505005\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 25 -> loss: 0.2811172604560852\u001b[0m\n",
      "\u001b[34mINFO:__main__:Created a folder at /opt/ml/model/code/!\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving models files to /opt/ml/model/code/\u001b[0m\n",
      "\u001b[34m2023-01-25 16:38:44,878 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-25 16:39:00 Completed - Training job completed\n",
      "Training seconds: 73\n",
      "Billable seconds: 73\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\"epochs\": 25, \"batch_size\": 128, \"learning_rate\": 0.01}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local\"\n",
    "    inputs = {\"train\": f\"file://{train_dir}\", \"test\": f\"file://{test_dir}\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.c5.xlarge\"\n",
    "    inputs = {\"train\": numpy_train_s3_uri, \"test\": numpy_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_pytorch_without_dependencies.py\",\n",
    "    \"source_dir\": \"pytorch_script\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"pytorch-model\",\n",
    "    \"framework_version\": \"1.5\",\n",
    "    \"py_version\": \"py3\",\n",
    "}\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, after the estimator finishes training, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: pytorch-model-2023-01-25-16-39-42-860\n",
      "INFO:sagemaker:Creating endpoint-config with name pytorch-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name pytorch-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=pytorch_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    pytorch_predictor = estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=pytorch_endpoint_name\n",
    "    )\n",
    "else:\n",
    "    pytorch_predictor = Predictor(\n",
    "        endpoint_name=\"pytorch-endpoint\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.702870845794678"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_predictor.serializer = JSONSerializer()\n",
    "pytorch_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "pytorch_predictor.predict(x_test.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00100000e+03, 2.95349534e+03, 4.00000000e+00, 1.50000000e+00,\n",
       "       9.80000000e-01, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third \"level\" of script mode is the ability to bring your own libraries and dependencies to support custom functionality within your models, training jobs, and inference processes. This supercharges your customization options, and allows you to import libraries you have created yourself or Python packages hosted on PyPi.\n",
    "\n",
    "Perhaps the number of Python files you have is becoming unwieldy now or you want more organization. In this scenario, you might be tempted to create your own Python library. Or maybe you wish to implement a function not currently supported by SageMaker in the training phase (such as  k-fold cross validation).\n",
    "\n",
    "Script Mode supports adding custom libraries and those libraries don't have to be in the same directory as your entry point Python script. You simply need to stipulate the custom library or other dependencies when defining your SageMaker training estimator (illustrated below). SageMaker will copy the library folder to the same folder where the entry point script is located when the training job is kicked off.\n",
    "\n",
    "In this example, we implement k-fold cross validation for an XGBoost model using a custom built library called `my_custom_library`. While XGBoost is supported \"out-of-the-box\" on SageMaker, that version does not support k-fold cross validation for training. Thus we use script mode to leverage the supported XGBoost container and the concomitant flexibility to include our custom libraries and dependencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgboost-model-2023-01-25-16-42-15-727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-25 16:42:16 Starting - Starting the training job...\n",
      "2023-01-25 16:42:32 Starting - Preparing the instances for training......\n",
      "2023-01-25 16:43:24 Downloading - Downloading input data.....\u001b[34m[2023-01-25 16:44:26.551 ip-10-0-233-202.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Invoking user training script.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Module train_deploy_xgboost_with_dependencies does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating setup.cfg\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train-deploy-xgboost-with-dependencies\n",
      "  Building wheel for train-deploy-xgboost-with-dependencies (setup.py): started\n",
      "  Building wheel for train-deploy-xgboost-with-dependencies (setup.py): finished with status 'done'\n",
      "  Created wheel for train-deploy-xgboost-with-dependencies: filename=train_deploy_xgboost_with_dependencies-1.0.0-py2.py3-none-any.whl size=6205 sha256=95e086ba7e9c337d672ed974a4c320e3731feb8598992b9c538bc32ceadb5945\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-l7k7_s3h/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built train-deploy-xgboost-with-dependencies\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train-deploy-xgboost-with-dependencies\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-deploy-xgboost-with-dependencies-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"K\": 5,\n",
      "        \"num_round\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"xgboost-model-2023-01-25-16-42-15-727\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-512949343409/xgboost-model-2023-01-25-16-42-15-727/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy_xgboost_with_dependencies\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy_xgboost_with_dependencies.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"K\":5,\"num_round\":6}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy_xgboost_with_dependencies.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy_xgboost_with_dependencies\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-512949343409/xgboost-model-2023-01-25-16-42-15-727/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"K\":5,\"num_round\":6},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"xgboost-model-2023-01-25-16-42-15-727\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-512949343409/xgboost-model-2023-01-25-16-42-15-727/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy_xgboost_with_dependencies\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy_xgboost_with_dependencies.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"-K\",\"5\",\"--num_round\",\"6\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_K=5\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=6\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python3.6/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m train_deploy_xgboost_with_dependencies -K 5 --num_round 6\u001b[0m\n",
      "\u001b[34m[16:44:28] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.40007#011validation-rmse:0.40007\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.32014#011validation-rmse:0.32014\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.25618#011validation-rmse:0.25618\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.20500#011validation-rmse:0.20500\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.16404#011validation-rmse:0.16404\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.13126#011validation-rmse:0.13126\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.10504#011validation-rmse:0.10504\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.08405#011validation-rmse:0.08405\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.06726#011validation-rmse:0.06726\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.05382#011validation-rmse:0.05382\u001b[0m\n",
      "\u001b[34m[16:44:28] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.40007#011validation-rmse:0.40007\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.32014#011validation-rmse:0.32014\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.25618#011validation-rmse:0.25618\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.20500#011validation-rmse:0.20500\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.16404#011validation-rmse:0.16404\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.13126#011validation-rmse:0.13126\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.10504#011validation-rmse:0.10504\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.08405#011validation-rmse:0.08405\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.06726#011validation-rmse:0.06726\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.05382#011validation-rmse:0.05382\u001b[0m\n",
      "\u001b[34m[16:44:28] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.40007#011validation-rmse:0.40007\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.32014#011validation-rmse:0.32014\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.25618#011validation-rmse:0.25618\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.20500#011validation-rmse:0.20500\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.16404#011validation-rmse:0.16404\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.13126#011validation-rmse:0.13127\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.10504#011validation-rmse:0.10504\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.08405#011validation-rmse:0.08405\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.06726#011validation-rmse:0.06726\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.05382#011validation-rmse:0.05382\u001b[0m\n",
      "\u001b[34m[16:44:28] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.40007#011validation-rmse:0.40007\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.32014#011validation-rmse:0.32014\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.25618#011validation-rmse:0.25618\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.20500#011validation-rmse:0.20499\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.16404#011validation-rmse:0.16404\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.13126#011validation-rmse:0.13126\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.10504#011validation-rmse:0.10504\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.08405#011validation-rmse:0.08405\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.06726#011validation-rmse:0.06726\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.05382#011validation-rmse:0.05382\u001b[0m\n",
      "\u001b[34m[16:44:28] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.40007#011validation-rmse:0.40007\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.32014#011validation-rmse:0.32014\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.25618#011validation-rmse:0.25618\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.20500#011validation-rmse:0.20500\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.16404#011validation-rmse:0.16404\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.13126#011validation-rmse:0.13126\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.10504#011validation-rmse:0.10504\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.08405#011validation-rmse:0.08405\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.06726#011validation-rmse:0.06726\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.05382#011validation-rmse:0.05382\u001b[0m\n",
      "\u001b[34mRMSE average across folds: 0.053822\u001b[0m\n",
      "\n",
      "2023-01-25 16:44:47 Training - Training image download completed. Training in progress.\n",
      "2023-01-25 16:44:47 Uploading - Uploading generated training model\n",
      "2023-01-25 16:44:47 Completed - Training job completed\n",
      "Training seconds: 82\n",
      "Billable seconds: 82\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\"num_round\": 6, \"K\": 5}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local\"\n",
    "    inputs = {\"train\": f\"file://{train_dir}\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.c5.xlarge\"\n",
    "    inputs = {\"train\": csv_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_xgboost_with_dependencies.py\",\n",
    "    \"source_dir\": \"xgboost_script\",\n",
    "    \"dependencies\": [\"my_custom_library\"],\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"xgboost-model\",\n",
    "    \"framework_version\": \"1.0-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "}\n",
    "\n",
    "estimator = XGBoost(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train the model with k-fold cross validation, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=xgboost_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: xgboost-model-2023-01-25-16-44-59-131\n",
      "INFO:sagemaker:Creating endpoint-config with name xgboost-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=xgboost_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    xgboost_predictor = estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=xgboost_endpoint_name\n",
    "    )\n",
    "else:\n",
    "    xgboost_predictor = Predictor(\n",
    "        endpoint_name=\"xgboost-endpoint\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.053864985704422]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_predictor.serializer = CSVSerializer()\n",
    "xgboost_predictor.deserializer = JSONDeserializer()\n",
    "xgboost_predictor.predict(x_test.values[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00100000e+03, 2.95349534e+03, 4.00000000e+00, 1.50000000e+00,\n",
       "       9.80000000e-01, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = (\n",
    "    [sklearn_endpoint_name, sklearn_predictor],\n",
    "    [pytorch_endpoint_name, pytorch_predictor],\n",
    "    [xgboost_endpoint_name, xgboost_predictor],\n",
    ")\n",
    "\n",
    "for resource in resources:\n",
    "    existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "        NameContains=resource[0], MaxResults=30\n",
    "    )[\"Endpoints\"]\n",
    "    if existing_endpoints:\n",
    "        resource[1].delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
