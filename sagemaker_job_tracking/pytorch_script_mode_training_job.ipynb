{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7e4dca-3653-4bf6-80a0-d964492d1d91",
   "metadata": {},
   "source": [
    "# Track an experiment while training a Pytorch model with a SageMaker Training Job\n",
    "\n",
    "This notebook shows how you can use the SageMaker SDK to track a Machine Learning experiment using a Pytorch model trained in a SageMaker Training Job with Script mode, where you will provide the model script file.\n",
    "\n",
    "We introduce two concepts in this notebook -\n",
    "\n",
    "* *Experiment:* An experiment is a collection of runs. When you initialize a run in your training loop, you include the name of the experiment that the run belongs to. Experiment names must be unique within your AWS account. \n",
    "* *Run:* A run consists of all the inputs, parameters, configurations, and results for one iteration of model training. Initialize an experiment run for tracking a training job with Run(). \n",
    "\n",
    "\n",
    "To execute this notebook in SageMaker Studio, you should select the `PyTorch 1.12 Python 3.8 CPU Optimizer` image.\n",
    "\n",
    "\n",
    "You can track artifacts for experiments, including datasets, algorithms, hyperparameters and metrics. Experiments executed on SageMaker such as SageMaker training jobs are automatically tracked and any existen SageMaker experiment on your AWS account is automatically migrated to the new UI version.\n",
    "\n",
    "In this notebook we will demonstrate the capabilities through an MNIST handwritten digits classification example. The notebook is organized as follow:\n",
    "\n",
    "1. Train a Convolutional Neural Network (CNN) Model and log the model training metrics\n",
    "1. Tune the hyperparameters that configures the number of hidden channels and the optimized in the model. Track teh parameter's configuration, resulting model loss and accuracy and automatically plot a confusion matrix using the Experiments capabilities of the SageMaker SDK.\n",
    "1. Analyse your model results and plot graphs comparing your model different runs generated from the tunning step 3.\n",
    "\n",
    "## Runtime\n",
    "This notebook takes approximately 45 minutes to run.\n",
    "\n",
    "## Contents\n",
    "1. [Install modules](#Install-modules)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create model training script](#Create-model-training-script)\n",
    "1. [Train model with Run context](#Train-model-with-Run-context)\n",
    "1. [Contact](#Contact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1141d3f8-45ed-4a56-8651-8964446befac",
   "metadata": {},
   "source": [
    "## Install modules\n",
    "\n",
    "Let's ensure we have the latest SageMaker SDK available, including the SageMaker Experiments functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d538673-0c04-455a-83a4-157d72edd3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6534d0-316b-4227-af84-37349d39c81b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (1.26.58)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.58 in /opt/conda/lib/python3.7/site-packages (from boto3) (1.29.58)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.58->boto3) (1.26.13)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.58->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.58->boto3) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.130.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (22.1.0)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.19.6)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.26.58)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.58 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.58)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.58->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m972.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision) (4.4.0)\n",
      "Requirement already satisfied: torch==1.13.1 in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (59.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# update boto3 and sagemaker to ensure latest SDK version\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade boto3\n",
    "!{sys.executable} -m pip install --upgrade sagemaker\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368d208-aebb-4844-bf27-2b2e373ef3d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and set logging and experiment configuration\n",
    "\n",
    "SageMaker Experiments now provides the `Run` class that allows you to create a new experiment run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "037c2813-b191-4420-b37b-9c6d1cbb8057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-vs-pytorch-script-mode-experiment\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "role = get_execution_role()\n",
    "region = Session().boto_session.region_name\n",
    "\n",
    "\n",
    "# set new experiment configuration\n",
    "experiment_name = \"tensorflow-vs-pytorch-script-mode-experiment\"\n",
    "# experiment_name = unique_name_from_base(\"training-job-experiment\")\n",
    "run_name = \"experiment-run-example\"\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd3511c-fdd9-4d4b-bf89-1804814801ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::512949343409:role/TeamRole'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ddbb88-ecbb-4bb2-8541-08a74001f64a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024c0e09-b08c-48c0-bae8-ef142c3d5e70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-vs-pytorch-script-mode-experiment'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc0054-d7dd-4ec8-b1e9-0b292fc7b1c0",
   "metadata": {},
   "source": [
    "## Create model training script\n",
    "Let's create `mnist.py`, the pytorch script file to train our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49259885-530e-4675-bd72-e21934014e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c6e08a-92d3-4819-a080-4858337813cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./script/mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/mnist.py\n",
    "# ensure that the latest version of the SageMaker SDK is available\n",
    "import os\n",
    "\n",
    "os.system(\"pip install -U sagemaker\")\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from os.path import join\n",
    "import boto3\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.experiments.run import load_run\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "if \"SAGEMAKER_METRICS_DIRECTORY\" in os.environ:\n",
    "    log_file_handler = logging.FileHandler(\n",
    "        join(os.environ[\"SAGEMAKER_METRICS_DIRECTORY\"], \"metrics.json\")\n",
    "    )\n",
    "    formatter = logging.Formatter(\n",
    "        \"{'time':'%(asctime)s', 'name': '%(name)s', \\\n",
    "        'level': '%(levelname)s', 'message': '%(message)s'}\",\n",
    "        style=\"%\",\n",
    "    )\n",
    "    log_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(log_file_handler)\n",
    "\n",
    "# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, kernel_size, drop_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, hidden_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, 20, kernel_size=kernel_size)\n",
    "        self.conv2_drop = torch.nn.Dropout2d(p=drop_out)\n",
    "        self.fc1 = torch.nn.Linear(320, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        )\n",
    "        x = x.view(-1, 320)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def log_performance(model, data_loader, device, epoch, run, metric_type=\"Test\"):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(data_loader.dataset)\n",
    "    # log metrics\n",
    "    run.log_metric(name=metric_type + \":loss\", value=loss, step=epoch)\n",
    "    run.log_metric(name=metric_type + \":accuracy\", value=accuracy, step=epoch)\n",
    "    logger.info(\n",
    "        \"{} Average loss: {:.4f}, {} Accuracy: {:.4f}%;\\n\".format(\n",
    "            metric_type, loss, metric_type, accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    run, train_set, test_set, data_dir=\"mnist_data\", optimizer=\"sgd\", epochs=10, hidden_channels=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Function that trains the CNN classifier to identify the MNIST digits.\n",
    "    Args:\n",
    "        run (sagemaker.experiments.run.Run): SageMaker Experiment run object\n",
    "        train_set (torchvision.datasets.mnist.MNIST): train dataset\n",
    "        test_set (torchvision.datasets.mnist.MNIST): test dataset\n",
    "        data_dir (str): local directory where the MNIST datasource is stored\n",
    "        optimizer (str): the optimization algorthm to use for training your CNN\n",
    "                         available options are sgd and adam\n",
    "        epochs (int): number of complete pass of the training dataset through the algorithm\n",
    "        hidden_channels (int): number of hidden channels in your model\n",
    "    \"\"\"\n",
    "\n",
    "    # log the parameters of your model\n",
    "    run.log_parameter(\"device\", \"cpu\")\n",
    "    run.log_parameters(\n",
    "        {\n",
    "            \"data_dir\": data_dir,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"epochs\": epochs,\n",
    "            \"hidden_channels\": hidden_channels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # train the model on the CPU (no GPU)\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=True)\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "            len(test_loader.sampler),\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    model = Net(hidden_channels, kernel_size=5, drop_out=0.5).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    momentum = 0.5\n",
    "    lr = 0.01\n",
    "    log_interval = 100\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Training Epoch:\", epoch)\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                logger.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)], Train Loss: {:.6f};\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "        log_performance(model, train_loader, device, epoch, run, \"Train\")\n",
    "        log_performance(model, test_loader, device, epoch, run, \"Test\")\n",
    "    # log confusion matrix\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            run.log_confusion_matrix(target, pred, \"Confusion-Matrix-Test-Data\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    hidden_channels = int(os.environ.get(\"hidden_channels\", \"5\"))\n",
    "    kernel_size = int(os.environ.get(\"kernel_size\", \"5\"))\n",
    "    dropout = float(os.environ.get(\"dropout\", \"0.5\"))\n",
    "    model = torch.nn.DataParallel(Net(hidden_channels, kernel_size, dropout))\n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "        return model.to(device)\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, run):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"sgd\", help=\"optimizer for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--hidden_channels\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number of channels in hidden conv layer\",\n",
    "    )\n",
    "    parser.add_argument(\"--region\", type=str, default=\"us-east-2\", help=\"SageMaker Region\")\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    # download the dataset\n",
    "    # this will not only download data to ./mnist folder, but also load and transform (normalize) them\n",
    "    datasets.MNIST.urls = [\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\",\n",
    "    ]\n",
    "    train_set = datasets.MNIST(\n",
    "        \"mnist_data\",\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    test_set = datasets.MNIST(\n",
    "        \"mnist_data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    session = Session(boto3.session.Session(region_name=args.region))\n",
    "    with load_run(sagemaker_session=session) as run:\n",
    "        run.log_parameters(\n",
    "            {\"num_train_samples\": len(train_set.data), \"num_test_samples\": len(test_set.data)}\n",
    "        )\n",
    "        for f in os.listdir(train_set.raw_folder):\n",
    "            print(\"Logging\", train_set.raw_folder + \"/\" + f)\n",
    "            run.log_file(train_set.raw_folder + \"/\" + f, name=f, is_output=False)\n",
    "        model = train_model(\n",
    "            run,\n",
    "            train_set,\n",
    "            test_set,\n",
    "            data_dir=\"mnist_data\",\n",
    "            optimizer=args.optimizer,\n",
    "            epochs=args.epochs,\n",
    "            hidden_channels=args.hidden_channels,\n",
    "        )\n",
    "        save_model(model, args.model_dir, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342b174-1d33-4b27-a7cd-28571f1a1507",
   "metadata": {},
   "source": [
    "The cell above saves the `mnist.py` file to our script folder. The file implements the code necessary to train our PyTorch model in SageMaker, using the SageMaker PyTorch image. It uses the `load_run` function to automatically detect the experiment configuration and `run.log_parameter`, `run.log_parameters`, `run.log_file`, `run.log_metric` and `run.log_confusion_matrix` to track the model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1913d9-fe5f-4cf1-aca6-c6f6c12bd21c",
   "metadata": {},
   "source": [
    "## Train model with Run context\n",
    "\n",
    "Let's now train the model with passing the experiement run context to the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f266e0-d73d-452c-a3ed-51b0fc48075d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-27-15-00-13-270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-27 15:00:13 Starting - Starting the training job...\n",
      "2023-01-27 15:00:36 Starting - Preparing the instances for training......\n",
      "2023-01-27 15:01:28 Downloading - Downloading input data...\n",
      "2023-01-27 15:01:48 Training - Downloading the training image...\n",
      "2023-01-27 15:02:23 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,642 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,644 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,646 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,654 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,657 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,845 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,848 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,859 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,861 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,872 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,874 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:27,884 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 5,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"region\": \"us-east-1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-27-15-00-13-270\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-512949343409/pytorch-training-2023-01-27-15-00-13-270/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"us-east-1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-512949343409/pytorch-training-2023-01-27-15-00-13-270/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"us-east-1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-27-15-00-13-270\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-512949343409/pytorch-training-2023-01-27-15-00-13-270/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"5\",\"--optimizer\",\"adam\",\"--region\",\"us-east-1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=us-east-1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region us-east-1\u001b[0m\n",
      "\u001b[34m2023-01-27 15:02:28,423 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.127.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.130.0.tar.gz (662 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 662.3/662.3 kB 53.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.45 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.45->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.130.0-py2.py3-none-any.whl size=899125 sha256=82eff92121fb61517024974884bfef1de42b83f6dc5483b98a315d1aa39c2538\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/58/4e/53/1cb3e4620b22ad08683ee485e153aa966b2556afc371cad5f6\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.127.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.127.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.127.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.130.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 313145571.56it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 245213955.11it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 175797951.11it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 29953661.58it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example) under experiment (tensorflow-vs-pytorch-script-mode-experiment) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-27 15:02:35.786 algo-1:44 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-01-27 15:02:35.955 algo-1:44 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-27 15:02:35.956 algo-1:44 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-27 15:02:35.957 algo-1:44 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-27 15:02:35.957 algo-1:44 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-27 15:02:35.957 algo-1:44 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 6\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2023-01-27 15:06:35,076 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-27 15:06:35,076 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-27 15:06:35,077 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-27 15:06:51 Uploading - Uploading generated training model\n",
      "2023-01-27 15:06:51 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 323\n",
      "Billable seconds: 323\n",
      "CPU times: user 1.55 s, sys: 161 ms, total: 1.71 s\n",
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start training job with experiment setting\n",
    "with Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=Session()) as run:\n",
    "    est = PyTorch(\n",
    "        entry_point=\"./script/mnist.py\",\n",
    "        role=role,\n",
    "        model_dir=False,\n",
    "        framework_version=\"1.12\",\n",
    "        py_version=\"py38\",\n",
    "        instance_type=\"ml.c5.xlarge\",\n",
    "        instance_count=1,\n",
    "        hyperparameters={\"epochs\": 10, \"hidden_channels\": 5, \"optimizer\": \"adam\", \"region\": region},\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )\n",
    "\n",
    "    est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcb80a89-5328-435a-b05f-8ab85504ab82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.pytorch.estimator.PyTorch at 0x7fa7ecfd2610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7635d5d8-785c-4eda-9628-b2d9c17aeef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(model_data=est.model_data, role=role, framework_version=\"1.12\", entry_point=\"./script/mnist.py\", py_version=\"py38\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c4fca-5128-4213-8065-cb68bd50b973",
   "metadata": {},
   "source": [
    "Checking the SageMaker Experiments UI, you can observe the Experiment run, populated with the metrics and parameters logged. We can also see the automatically generated outputs for the model data\n",
    "\n",
    "\n",
    "<img src=\"images/sm_training_exp_overview.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_inputs.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_parameters.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_metrics.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_outputs.png\" width=\"100%\" style=\"float: left;\" />"
   ]
  }
 ],
 "metadata": {
  "forced_instance_type": "ml.t3.medium",
  "forced_lcc_arn": "",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
